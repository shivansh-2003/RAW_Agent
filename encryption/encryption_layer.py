# encryption_layer.py

import os
import time
import uuid
import hmac
import hashlib
import base64
import json
import secrets
import binascii
import logging
from typing import Dict, List, Optional, Any, Tuple, Union
from datetime import datetime
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
from cryptography.hazmat.primitives import padding, hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
from cryptography.hazmat.backends import default_backend

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("encryption_layer.log")
    ]
)
logger = logging.getLogger("shadow_encryption_layer")

class LayeredCipherCodeSystem:
    """
    Layered Cipher Code (LCC) System for Project SHADOW
    
    The LCC system provides multi-layer encryption for sensitive communications
    between agents and the system, using a combination of:
    1. Quantum-resistant hashing
    2. Layered symmetric encryption
    3. One-time pads
    4. Temporal keys that change with each communication
    
    This implementation follows the protocol from the SECRET INFO MANUAL.
    """
    
    def __init__(self, quantum_hash=None, otp_generator=None, ghost_step=None):
        """Initialize the LCC System with optional component overrides"""
        # Initialize all components
        self.quantum_hash = quantum_hash or QuantumHashingModule()
        self.otp_generator = otp_generator or OneTimePadGenerator()
        self.ghost_step = ghost_step or GhostStepAlgorithm()
        
        # LCC-specific configuration
        self.layer_count = 3  # Default number of encryption layers
        self.key_refresh_interval = 300  # Seconds between key rotations
        self.version = "LCC-3.7"
        
        # Store the last used key material for verification
        self.key_history = {}
        
        logger.info("Layered Cipher Code System initialized")
    
    def encrypt(
        self, 
        plaintext: str, 
        agent_id: str, 
        clearance_level: int, 
        session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Encrypt a message using the LCC system
        
        Args:
            plaintext: Original text to encrypt
            agent_id: Identifier of the agent
            clearance_level: Agent's clearance level (1-5)
            session_id: Optional session identifier for key derivation
            
        Returns:
            Dictionary containing the encrypted message and metadata
        """
        try:
            # Generate a message ID and timestamp
            message_id = str(uuid.uuid4())
            timestamp = datetime.utcnow().isoformat()
            
            # Apply security level adaptations based on clearance
            security_level = min(clearance_level + 1, 5)  # Scale 2-5 based on clearance
            layer_count = min(security_level, self.layer_count)
            
            # 1. Generate a quantum hash signature
            quantum_signature = self.quantum_hash.generate_hash(
                f"{plaintext}:{agent_id}:{timestamp}", 
                security_level
            )
            
            # Convert plaintext to bytes if not already
            if isinstance(plaintext, str):
                plaintext_bytes = plaintext.encode('utf-8')
            else:
                plaintext_bytes = plaintext
            
            # 2. Apply layered encryption
            encrypted_data = plaintext_bytes
            layer_keys = []
            layer_ivs = []
            
            for i in range(layer_count):
                # Generate a unique key and IV for this layer
                key_material = self._derive_key_material(
                    agent_id=agent_id,
                    clearance_level=clearance_level,
                    layer=i,
                    timestamp=timestamp,
                    message_id=message_id,
                    session_id=session_id
                )
                
                # Split into key and IV
                key = key_material[:32]  # 256 bits for AES-256
                iv = key_material[32:48]  # 128 bits for IV
                
                # Store for inclusion in output
                layer_keys.append(base64.b64encode(key).decode('utf-8'))
                layer_ivs.append(base64.b64encode(iv).decode('utf-8'))
                
                # Apply padding
                padder = padding.PKCS7(128).padder()
                padded_data = padder.update(encrypted_data) + padder.finalize()
                
                # Encrypt with AES-256
                cipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())
                encryptor = cipher.encryptor()
                encrypted_data = encryptor.update(padded_data) + encryptor.finalize()
            
            # 3. Apply one-time pad (for highest security levels)
            otp_applied = False
            otp_id = None
            
            if clearance_level >= 4:
                otp_data = self.otp_generator.generate_pad(
                    agent_id=agent_id,
                    data_length=len(encrypted_data),
                    purpose="encrypt"
                )
                
                # XOR with one-time pad
                encrypted_data = bytes(a ^ b for a, b in zip(encrypted_data, otp_data["pad"]))
                otp_id = otp_data["pad_id"]
                otp_applied = True
            
            # 4. Apply Ghost-Step algorithm to remove digital fingerprints
            encrypted_data = self.ghost_step.apply(
                data=encrypted_data,
                agent_id=agent_id,
                clearance_level=clearance_level
            )
            
            # Encode the final encrypted data
            encoded_data = base64.b64encode(encrypted_data).decode('utf-8')
            
            # Store key material in history for potential verification
            self.key_history[message_id] = {
                "timestamp": timestamp,
                "keys": layer_keys,
                "ivs": layer_ivs,
                "otp_id": otp_id,
                "expires_at": datetime.utcnow().timestamp() + (3600 * 24)  # 24h expiry
            }
            
            # Create result
            result = {
                "version": self.version,
                "message_id": message_id,
                "timestamp": timestamp,
                "encrypted_data": encoded_data,
                "quantum_signature": quantum_signature,
                "security_level": security_level,
                "layer_count": layer_count,
                "otp_applied": otp_applied,
                "otp_id": otp_id if otp_applied else None,
                "metadata": {
                    "agent_level": clearance_level,
                    "ciphersuite": "AES-256-CBC",
                    "ghost_step_version": self.ghost_step.version
                }
            }
            
            return result
            
        except Exception as e:
            logger.error(f"LCC encryption error: {e}")
            raise
    
    def decrypt(
        self, 
        encrypted_payload: Dict[str, Any], 
        agent_id: str, 
        clearance_level: int,
        session_id: Optional[str] = None
    ) -> str:
        """
        Decrypt a message encrypted with the LCC system
        
        Args:
            encrypted_payload: Dictionary with encrypted data and metadata
            agent_id: Identifier of the agent
            clearance_level: Agent's clearance level (1-5)
            session_id: Optional session identifier for key derivation
            
        Returns:
            Decrypted plaintext
        """
        try:
            # Extract data from payload
            message_id = encrypted_payload.get("message_id")
            timestamp = encrypted_payload.get("timestamp")
            encoded_data = encrypted_payload.get("encrypted_data")
            layer_count = encrypted_payload.get("layer_count", self.layer_count)
            otp_applied = encrypted_payload.get("otp_applied", False)
            otp_id = encrypted_payload.get("otp_id")
            
            if not all([message_id, timestamp, encoded_data]):
                raise ValueError("Missing required fields in encrypted payload")
            
            # Decode the encrypted data
            encrypted_data = base64.b64decode(encoded_data)
            
            # 1. Reverse Ghost-Step algorithm
            encrypted_data = self.ghost_step.reverse(
                data=encrypted_data,
                agent_id=agent_id,
                clearance_level=clearance_level
            )
            
            # 2. Remove one-time pad if applied
            if otp_applied:
                if not otp_id:
                    raise ValueError("OTP was applied but no pad ID provided")
                
                otp_data = self.otp_generator.retrieve_pad(
                    pad_id=otp_id,
                    agent_id=agent_id,
                    data_length=len(encrypted_data),
                    purpose="decrypt"
                )
                
                # XOR to remove pad
                encrypted_data = bytes(a ^ b for a, b in zip(encrypted_data, otp_data["pad"]))
            
            # 3. Decrypt through each layer
            for i in range(layer_count - 1, -1, -1):  # Start from the outermost layer
                # Derive key material for this layer
                key_material = self._derive_key_material(
                    agent_id=agent_id,
                    clearance_level=clearance_level,
                    layer=i, 
                    timestamp=timestamp,
                    message_id=message_id,
                    session_id=session_id
                )
                
                # Split into key and IV
                key = key_material[:32]
                iv = key_material[32:48]
                
                # Decrypt with AES-256
                cipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())
                decryptor = cipher.decryptor()
                decrypted_data = decryptor.update(encrypted_data) + decryptor.finalize()
                
                # Remove padding
                unpadder = padding.PKCS7(128).unpadder()
                try:
                    encrypted_data = unpadder.update(decrypted_data) + unpadder.finalize()
                except Exception as e:
                    # If we can't unpad, this might be the innermost layer
                    if i == 0:
                        encrypted_data = decrypted_data
                    else:
                        raise
            
            # Convert bytes to string
            plaintext = encrypted_data.decode('utf-8')
            
            # 4. Verify quantum signature if provided
            if "quantum_signature" in encrypted_payload:
                expected_signature = self.quantum_hash.generate_hash(
                    f"{plaintext}:{agent_id}:{timestamp}", 
                    encrypted_payload.get("security_level", 3)
                )
                
                if expected_signature != encrypted_payload["quantum_signature"]:
                    logger.warning(f"Quantum signature verification failed for message {message_id}")
            
            return plaintext
            
        except Exception as e:
            logger.error(f"LCC decryption error: {e}")
            raise
    
    def verify_message(self, encrypted_payload: Dict[str, Any]) -> bool:
        """
        Verify the integrity of an encrypted message without decrypting it
        
        Args:
            encrypted_payload: Dictionary with encrypted data and metadata
            
        Returns:
            True if the message integrity is verified, False otherwise
        """
        try:
            # Extract required fields
            message_id = encrypted_payload.get("message_id")
            quantum_signature = encrypted_payload.get("quantum_signature")
            timestamp = encrypted_payload.get("timestamp")
            
            if not all([message_id, quantum_signature, timestamp]):
                return False
            
            # Check if message timestamp is recent (within 1 hour)
            try:
                msg_time = datetime.fromisoformat(timestamp)
                now = datetime.utcnow()
                time_diff = (now - msg_time).total_seconds()
                
                if time_diff > 3600 or time_diff < -60:  # Allow 1min clock skew
                    logger.warning(f"Message {message_id} timestamp is too old or in the future")
                    return False
            except:
                logger.warning(f"Invalid timestamp format in message {message_id}")
                return False
            
            # Simple integrity check - in a real system, this would perform
            # more sophisticated verification of the quantum signature
            if len(quantum_signature) != 64:  # Expected length for a SHA-256 hex string
                return False
            
            # Check if this message_id has been seen before (replay protection)
            # This would typically be checked against a database
            
            return True
            
        except Exception as e:
            logger.error(f"Message verification error: {e}")
            return False
    
    def _derive_key_material(
        self, 
        agent_id: str, 
        clearance_level: int,
        layer: int, 
        timestamp: str,
        message_id: str,
        session_id: Optional[str] = None
    ) -> bytes:
        """
        Derive key material for a specific encryption layer
        
        Args:
            agent_id: Agent identifier
            clearance_level: Agent's clearance level
            layer: Encryption layer index
            timestamp: Message timestamp
            message_id: Unique message identifier
            session_id: Optional session identifier
            
        Returns:
            48 bytes of key material (32 for key, 16 for IV)
        """
        # Create a salt based on agent, layer, and timestamp
        salt_data = f"{agent_id}:{layer}:{timestamp}:{message_id}"
        if session_id:
            salt_data += f":{session_id}"
        
        salt = hashlib.sha256(salt_data.encode()).digest()
        
        # Use a fixed secret key (in a real implementation, this would be stored securely)
        # This is just a demonstration - in production, use a proper key management system
        base_key = os.environ.get("LCC_BASE_KEY", "5ecret-pr0ject-sh@d0w-key-42").encode()
        
        # Add agent-specific entropy
        agent_factor = f"{agent_id}:{clearance_level}".encode()
        
        # Use PBKDF2 to derive key material
        kdf = PBKDF2HMAC(
            algorithm=hashes.SHA256(),
            length=48,  # 32 bytes for key + 16 bytes for IV
            salt=salt,
            iterations=10000 + (clearance_level * 1000),  # Higher clearance = more iterations
            backend=default_backend()
        )
        
        # Derive the key material
        key_material = kdf.derive(base_key + agent_factor)
        
        return key_material


class QuantumHashingModule:
    """
    Quantum Hashing Module for Project SHADOW
    
    This module provides enhanced hashing functions that simulate
    quantum-resistant properties for secure message verification.
    """
    
    def __init__(self):
        """Initialize the Quantum Hashing Module"""
        self.version = "QHM-2.3"
        self.hash_variants = {
            1: "basic",      # SHA-256
            2: "enhanced",   # SHA-256 + HMAC
            3: "advanced",   # SHA-3 + HMAC
            4: "quantum",    # SHA-3 + Multiple rounds
            5: "zeta"        # SHA-3 + Temporal factors + Multiple rounds
        }
        
        # Initialize salt secrets (would be stored securely in production)
        self.salt_secret = os.environ.get("QHM_SALT_SECRET", "qu@ntum-h@sh1ng-s@lt-42")
        
        logger.info("Quantum Hashing Module initialized")
    
    def generate_hash(self, data: str, security_level: int = 3) -> str:
        """
        Generate a hash for the given data at specified security level
        
        Args:
            data: The data to hash
            security_level: Security level (1-5)
            
        Returns:
            Hexadecimal hash string
        """
        # Ensure valid security level
        level = max(1, min(5, security_level))
        
        # Get hash variant for this security level
        variant = self.hash_variants[level]
        
        # Add temporal factor for higher security levels
        if level >= 3:
            # Add hourly time factor (changes every hour)
            hour_factor = datetime.utcnow().strftime("%Y%m%d%H")
            data = f"{data}:{hour_factor}"
        
        if level >= 5:
            # Add minute factor for highest security (changes every minute)
            minute_factor = datetime.utcnow().strftime("%M")
            data = f"{data}:{minute_factor}"
        
        # Generate hash based on variant
        if variant == "basic":
            # Simple SHA-256
            return hashlib.sha256(data.encode()).hexdigest()
            
        elif variant == "enhanced":
            # HMAC with SHA-256
            return hmac.new(
                self.salt_secret.encode(), 
                data.encode(), 
                hashlib.sha256
            ).hexdigest()
            
        elif variant == "advanced":
            # SHA3-256 with HMAC
            sha3_hash = hashlib.sha3_256(data.encode()).digest()
            return hmac.new(
                self.salt_secret.encode(),
                sha3_hash,
                hashlib.sha3_256
            ).hexdigest()
            
        elif variant == "quantum":
            # Multiple rounds of SHA3-256
            current_hash = data.encode()
            for i in range(3):  # 3 rounds
                current_hash = hashlib.sha3_256(current_hash).digest()
            
            return current_hash.hex()
            
        elif variant == "zeta":
            # Advanced multi-round with variable iteration count
            # Based on data content to prevent precomputation attacks
            data_seed = int(hashlib.md5(data.encode()).hexdigest(), 16) % 10
            iterations = 3 + data_seed  # 3-12 rounds
            
            # Initial salting
            salted_data = f"{self.salt_secret}:{data}"
            current_hash = salted_data.encode()
            
            # Multiple rounds with feedback
            for i in range(iterations):
                # Each round incorporates the iteration number
                round_data = current_hash + str(i).encode()
                current_hash = hashlib.sha3_256(round_data).digest()
                
                # Add intermediate binding every other round
                if i % 2 == 1:
                    # Bind with original data to prevent length extension attacks
                    current_hash = hashlib.sha3_256(current_hash + data.encode()).digest()
            
            return current_hash.hex()
        
        # Fallback to SHA-256 if unknown variant
        return hashlib.sha256(data.encode()).hexdigest()
    
    def verify_hash(self, data: str, hash_value: str, security_level: int = 3) -> bool:
        """
        Verify a hash against the provided data
        
        Args:
            data: The data to verify
            hash_value: The hash to verify against
            security_level: Security level (1-5)
            
        Returns:
            True if hash matches, False otherwise
        """
        expected_hash = self.generate_hash(data, security_level)
        
        # Use constant-time comparison to prevent timing attacks
        return secrets.compare_digest(expected_hash, hash_value)
    
    def chain_hash(self, previous_hash: str, data: str, security_level: int = 3) -> str:
        """
        Generate a chained hash combining previous hash and new data
        
        Args:
            previous_hash: Previous hash in the chain
            data: New data to incorporate
            security_level: Security level (1-5)
            
        Returns:
            New hash in the chain
        """
        combined_data = f"{previous_hash}:{data}"
        return self.generate_hash(combined_data, security_level)


class OneTimePadGenerator:
    """
    One-Time Pad Generator and Manager for Project SHADOW
    
    Generates and manages one-time pads for the highest level of
    encryption security in the system.
    """
    
    def __init__(self):
        """Initialize the One-Time Pad Generator"""
        self.version = "OTP-2.1"
        
        # Storage for tracking used pads (in production, use a database)
        self.used_pads = set()
        self.pad_registry = {}  # pad_id -> metadata
        
        # Max size for OTP in bytes (adjustable based on needs)
        self.max_pad_size = 1024 * 1024  # 1MB
        
        logger.info("One-Time Pad Generator initialized")
    
    def generate_pad(
        self, 
        agent_id: str, 
        data_length: int, 
        purpose: str = "encrypt"
    ) -> Dict[str, Any]:
        """
        Generate a one-time pad for encryption or verification
        
        Args:
            agent_id: Agent identifier
            data_length: Length of data to generate pad for
            purpose: Purpose of the pad ("encrypt", "verify", etc.)
            
        Returns:
            Dictionary with pad data and metadata
        """
        if data_length > self.max_pad_size:
            raise ValueError(f"Requested pad size {data_length} exceeds maximum {self.max_pad_size}")
        
        # Generate a unique pad ID
        pad_id = str(uuid.uuid4())
        
        # Generate random pad of required length
        pad = os.urandom(data_length)
        
        # Store pad metadata (in production, would be stored securely)
        timestamp = datetime.utcnow()
        self.pad_registry[pad_id] = {
            "agent_id": agent_id,
            "created_at": timestamp,
            "length": data_length,
            "purpose": purpose,
            "used": False
        }
        
        # Return the pad and its metadata
        return {
            "pad_id": pad_id,
            "pad": pad,
            "created_at": timestamp,
            "metadata": {
                "agent_id": agent_id,
                "purpose": purpose
            }
        }
    
    def retrieve_pad(
        self, 
        pad_id: str, 
        agent_id: str, 
        data_length: int, 
        purpose: str = "decrypt"
    ) -> Dict[str, Any]:
        """
        Retrieve a previously generated one-time pad
        
        Args:
            pad_id: ID of the pad to retrieve
            agent_id: Agent identifier (for verification)
            data_length: Expected length of the pad
            purpose: Purpose for retrieval
            
        Returns:
            Dictionary with pad data and metadata
        """
        # In a real implementation, this would retrieve from secure storage
        # For demo, regenerate based on pad_id
        
        if pad_id in self.used_pads:
            raise ValueError(f"Pad {pad_id} has already been used")
        
        # Check if pad exists in registry
        if pad_id not in self.pad_registry:
            raise ValueError(f"Pad {pad_id} not found")
        
        # Check if pad belongs to this agent
        pad_info = self.pad_registry[pad_id]
        if pad_info["agent_id"] != agent_id:
            raise ValueError(f"Pad {pad_id} does not belong to agent {agent_id}")
        
        # Check if pad length matches
        if pad_info["length"] != data_length:
            raise ValueError(f"Pad length mismatch: expected {pad_info['length']}, got {data_length}")
        
        # Mark pad as used
        self.used_pads.add(pad_id)
        self.pad_registry[pad_id]["used"] = True
        
        # In a real system, we would retrieve the actual pad from secure storage
        # For this demo, we'll deterministically regenerate it (NEVER do this in production)
        
        # Use pad_id as seed to generate the same pad
        seed = int(pad_id.replace('-', ''), 16) % (2**32)
        
        import random
        rng = random.Random(seed)
        pad = bytes(rng.getrandbits(8) for _ in range(data_length))
        
        return {
            "pad_id": pad_id,
            "pad": pad,
            "retrieved_at": datetime.utcnow(),
            "metadata": pad_info
        }
    
    def verify_pad_status(self, pad_id: str) -> Dict[str, Any]:
        """
        Verify the status of a one-time pad
        
        Args:
            pad_id: ID of the pad to check
            
        Returns:
            Status information for the pad
        """
        if pad_id not in self.pad_registry:
            return {"exists": False, "error": "Pad not found"}
        
        pad_info = self.pad_registry[pad_id]
        used = pad_id in self.used_pads or pad_info.get("used", False)
        
        age_seconds = (datetime.utcnow() - pad_info["created_at"]).total_seconds()
        
        return {
            "exists": True,
            "used": used,
            "age_seconds": age_seconds,
            "agent_id": pad_info["agent_id"],
            "purpose": pad_info["purpose"]
        }
    
    def destroy_pad(self, pad_id: str) -> bool:
        """
        Explicitly destroy a one-time pad after use
        
        Args:
            pad_id: ID of the pad to destroy
            
        Returns:
            True if pad was destroyed, False otherwise
        """
        if pad_id in self.pad_registry:
            # Mark as used
            self.used_pads.add(pad_id)
            
            # Remove from registry
            del self.pad_registry[pad_id]
            
            # In a real system, securely wipe the pad data
            
            return True
        
        return False


class GhostStepAlgorithm:
    """
    Ghost-Step Algorithm for Project SHADOW
    
    This algorithm applies transformations to remove digital fingerprints
    and forensic traces from encrypted data, making analysis and attribution
    more difficult.
    """
    
    def __init__(self):
        """Initialize the Ghost-Step Algorithm"""
        self.version = "GSA-3.2"
        
        # Transformation strength by clearance level
        self.transform_strength = {
            1: 0.2,  # Minimal obfuscation for Level 1
            2: 0.4,  # Light obfuscation for Level 2
            3: 0.6,  # Medium obfuscation for Level 3
            4: 0.8,  # Strong obfuscation for Level 4
            5: 1.0   # Maximum obfuscation for Level 5
        }
        
        logger.info("Ghost-Step Algorithm initialized")
    
    def apply(self, data: bytes, agent_id: str, clearance_level: int) -> bytes:
        """
        Apply the Ghost-Step Algorithm to remove digital fingerprints
        
        Args:
            data: Data to transform
            agent_id: Agent identifier
            clearance_level: Agent's clearance level
            
        Returns:
            Transformed data with reduced fingerprinting
        """
        # Get transformation strength based on clearance level
        strength = self.transform_strength.get(clearance_level, 0.5)
        
        # Phase 1: Temporal key derivation
        # Create a time-based key that changes throughout the day
        # This prevents correlation of traffic patterns
        time_key = self._generate_temporal_key(agent_id)
        
        # Phase 2: Apply transformations based on clearance level
        if clearance_level <= 2:
            # Basic transformation - simple XOR with time key
            transformed_data = self._apply_basic_transform(data, time_key)
        elif clearance_level <= 4:
            # Advanced transformation - includes block shuffling
            transformed_data = self._apply_advanced_transform(data, time_key, strength)
        else:
            # Maximum transformation - includes structural obfuscation
            transformed_data = self._apply_maximum_transform(data, time_key, agent_id)
        
        # Phase 3: Add noise patterns to defeat statistical analysis
        # Higher clearance levels get more sophisticated noise patterns
        transformed_data = self._add_noise_patterns(transformed_data, clearance_level)
        
        # Phase 4: Normalize data length to standard blocks
        # This prevents identifying messages by their size
        transformed_data = self._normalize_length(transformed_data, clearance_level)
        
        return transformed_data
    
    def reverse(self, data: bytes, agent_id: str, clearance_level: int) -> bytes:
        """
        Reverse the Ghost-Step Algorithm transformations
        
        Args:
            data: Transformed data
            agent_id: Agent identifier
            clearance_level: Agent's clearance level
            
        Returns:
            Original data with fingerprinting removed
        """
        # Get transformation strength based on clearance level
        strength = self.transform_strength.get(clearance_level, 0.5)
        
        # Phase 1: Temporal key derivation (same as in apply())
        time_key = self._generate_temporal_key(agent_id)
        
        # Phase 4 (Reverse): Remove length normalization
        data = self._remove_normalization(data, clearance_level)
        
        # Phase 3 (Reverse): Remove noise patterns
        data = self._remove_noise_patterns(data, clearance_level)
        
        # Phase 2 (Reverse): Reverse transformations based on clearance level
        if clearance_level <= 2:
            # Reverse basic transformation - simple XOR with time key
            original_data = self._apply_basic_transform(data, time_key)  # XOR is its own inverse
        elif clearance_level <= 4:
            # Reverse advanced transformation
            original_data = self._reverse_advanced_transform(data, time_key, strength)
        else:
            # Reverse maximum transformation
            original_data = self._reverse_maximum_transform(data, time_key, agent_id)
        
        return original_data
    
    def _generate_temporal_key(self, agent_id: str) -> bytes:
        """Generate a time-based key that changes every hour"""
        current_hour = datetime.utcnow().strftime("%Y%m%d%H")
        key_material = f"{agent_id}:{current_hour}:ghost-step-v{self.version}"
        
        # Generate 32 bytes of key material
        return hashlib.sha256(key_material.encode()).digest()
    
    def _apply_basic_transform(self, data: bytes, key: bytes) -> bytes:
        """Apply basic transformation (XOR with repeating key)"""
        # Create a repeating key of required length
        key_stream = bytearray()
        while len(key_stream) < len(data):
            key_stream.extend(key)
        key_stream = bytes(key_stream[:len(data)])
        
        # XOR data with key stream
        return bytes(a ^ b for a, b in zip(data, key_stream))
    
    def _apply_advanced_transform(self, data: bytes, key: bytes, strength: float) -> bytes:
        """Apply advanced transformation including block shuffling"""
        # First apply basic XOR
        result = self._apply_basic_transform(data, key)
        
        # Block size for shuffling
        block_size = 16
        
        # Only shuffle if data is large enough
        if len(result) < block_size * 2:
            return result
        
        # Calculate number of full blocks
        num_blocks = len(result) // block_size
        
        # Prepare shuffled blocks
        shuffled = bytearray(len(result))
        
        # Generate a shuffling pattern based on key
        # We'll only shuffle a percentage of blocks based on strength
        import hashlib
        import struct
        
        # Create a seed from the key
        seed = int.from_bytes(hashlib.sha256(key).digest()[:4], byteorder='little')
        
        # Initialize random generator with seed
        import random
        rng = random.Random(seed)
        
        # Number of blocks to shuffle
        shuffle_count = int(num_blocks * strength)
        
        # Create mapping for block shuffling
        block_map = list(range(num_blocks))
        for i in range(shuffle_count):
            # Swap two random blocks
            idx1 = rng.randrange(num_blocks)
            idx2 = rng.randrange(num_blocks)
            block_map[idx1], block_map[idx2] = block_map[idx2], block_map[idx1]
        
        # Apply mapping to shuffle blocks
        for i in range(num_blocks):
            source_idx = block_map[i]
            shuffled[i * block_size:(i + 1) * block_size] = result[source_idx * block_size:(source_idx + 1) * block_size]
        
        # Copy any remaining data
        if len(result) % block_size > 0:
            shuffled[num_blocks * block_size:] = result[num_blocks * block_size:]
        
        return bytes(shuffled)
    
    def _reverse_advanced_transform(self, data: bytes, key: bytes, strength: float) -> bytes:
        """Reverse advanced transformation"""
        # Block size for shuffling
        block_size = 16
        
        # Only unshuffle if data is large enough
        if len(data) < block_size * 2:
            return self._apply_basic_transform(data, key)  # XOR is its own inverse
        
        # Calculate number of full blocks
        num_blocks = len(data) // block_size
        
        # Prepare unshuffled blocks
        unshuffled = bytearray(len(data))
        
        # Generate the same shuffling pattern based on key
        import hashlib
        import struct
        
        # Create a seed from the key
        seed = int.from_bytes(hashlib.sha256(key).digest()[:4], byteorder='little')
        
        # Initialize random generator with seed
        import random
        rng = random.Random(seed)
        
        # Number of blocks to shuffle
        shuffle_count = int(num_blocks * strength)
        
        # Create mapping for block shuffling
        block_map = list(range(num_blocks))
        for i in range(shuffle_count):
            # Swap two random blocks (same as in apply)
            idx1 = rng.randrange(num_blocks)
            idx2 = rng.randrange(num_blocks)
            block_map[idx1], block_map[idx2] = block_map[idx2], block_map[idx1]
        
        # Apply inverse mapping to unshuffle blocks
        for i in range(num_blocks):
            target_idx = block_map[i]
            unshuffled[target_idx * block_size:(target_idx + 1) * block_size] = data[i * block_size:(i + 1) * block_size]
        
        # Copy any remaining data
        if len(data) % block_size > 0:
            unshuffled[num_blocks * block_size:] = data[num_blocks * block_size:]
        
        # Finally, apply basic XOR to reverse that transformation
        return self._apply_basic_transform(bytes(unshuffled), key)  # XOR is its own inverse
    
    def _apply_maximum_transform(self, data: bytes, key: bytes, agent_id: str) -> bytes:
        """Apply maximum transformation with structural obfuscation"""
        # First apply advanced transform
        result = self._apply_advanced_transform(data, key, 1.0)
        
        # Add additional structural obfuscation
        
        # 1. Split data into segments of varying sizes
        segments = []
        segment_size = 32  # Base segment size
        
        # Use agent_id to create a seed for segment sizing
        import hashlib
        agent_seed = int.from_bytes(hashlib.md5(agent_id.encode()).digest()[:4], byteorder='little')
        
        import random
        segment_rng = random.Random(agent_seed)
        
        # Split data into segments
        offset = 0
        while offset < len(result):
            # Vary segment size by Â±25%
            size_factor = 0.75 + (segment_rng.random() * 0.5)
            curr_size = max(16, int(segment_size * size_factor))
            curr_size = min(curr_size, len(result) - offset)
            
            segments.append(result[offset:offset+curr_size])
            offset += curr_size
        
        # 2. Add metadata to each segment for verification
        enhanced_segments = []
        for i, segment in enumerate(segments):
            # Add sequence number and checksum to each segment
            seq_num = i.to_bytes(4, byteorder='little')
            checksum = hashlib.md5(segment).digest()[:4]
            
            # Combine into enhanced segment: seq_num + checksum + segment
            enhanced_segment = seq_num + checksum + segment
            enhanced_segments.append(enhanced_segment)
        
        # 3. Mix up the order of segments (but remember the order)
        segment_order = list(range(len(enhanced_segments)))
        segment_rng.shuffle(segment_order)
        
        # Store the order in the first 'header' segment
        order_data = len(segment_order).to_bytes(4, byteorder='little')
        for idx in segment_order:
            order_data += idx.to_bytes(4, byteorder='little')
        
        # XOR the order data with a key derived from the main key
        order_key = hashlib.sha256(key + b"order").digest()[:len(order_data)]
        order_data_encrypted = bytes(a ^ b for a, b in zip(order_data, order_key))
        
        # 4. Combine everything into final result
        # Format: order_length(4) + encrypted_order + shuffled_segments
        final_result = len(order_data).to_bytes(4, byteorder='little') + order_data_encrypted
        
        # Add segments in shuffled order
        for idx in segment_order:
            final_result += enhanced_segments[idx]
        
        return final_result
    
    def _reverse_maximum_transform(self, data: bytes, key: bytes, agent_id: str) -> bytes:
        """Reverse maximum transformation"""
        # Extract order information
        order_length = int.from_bytes(data[:4], byteorder='little')
        order_data_encrypted = data[4:4+order_length]
        
        # Decrypt order data
        order_key = hashlib.sha256(key + b"order").digest()[:len(order_data_encrypted)]
        order_data = bytes(a ^ b for a, b in zip(order_data_encrypted, order_key))
        
        # Parse order information
        num_segments = int.from_bytes(order_data[:4], byteorder='little')
        segment_order = []
        for i in range(num_segments):
            idx = int.from_bytes(order_data[4+i*4:8+i*4], byteorder='little')
            segment_order.append(idx)
        
        # Extract the enhanced segments
        segment_data = data[4+order_length:]
        
        # Parse segments
        enhanced_segments = []
        offset = 0
        
        while offset < len(segment_data):
            # Each enhanced segment has a 8-byte header (4 seq + 4 checksum)
            if offset + 8 >= len(segment_data):
                break
                
            seq_num = int.from_bytes(segment_data[offset:offset+4], byteorder='little')
            checksum = segment_data[offset+4:offset+8]
            
            # Find the next segment boundary by checking sequence numbers
            next_offset = offset + 8
            while next_offset < len(segment_data) - 8:
                # Try to interpret as a potential sequence number
                try:
                    next_seq = int.from_bytes(segment_data[next_offset:next_offset+4], byteorder='little')
                    next_checksum = segment_data[next_offset+4:next_offset+8]
                    
                    # If this looks like a valid header and the next segment's sequence number
                    if next_seq < num_segments and next_seq != seq_num:
                        # Verify the current segment's checksum
                        segment_data_part = segment_data[offset+8:next_offset]
                        if hashlib.md5(segment_data_part).digest()[:4] == checksum:
                            break
                except:
                    pass
                    
                next_offset += 1
            
            # If we reached the end, this is the last segment
            if next_offset >= len(segment_data) - 8:
                segment_content = segment_data[offset+8:]
            else:
                segment_content = segment_data[offset+8:next_offset]
                
            enhanced_segments.append((seq_num, segment_content))
            offset = next_offset
        
        # Sort segments by sequence number
        enhanced_segments.sort(key=lambda x: x[0])
        
        # Combine segments
        result = b''.join(segment[1] for segment in enhanced_segments)
        
        # Finally, reverse the advanced transform
        return self._reverse_advanced_transform(result, key, 1.0)
    
    def _add_noise_patterns(self, data: bytes, clearance_level: int) -> bytes:
        """Add noise patterns to defeat statistical analysis"""
        # Different noise patterns based on clearance level
        if clearance_level <= 2:
            # Basic noise: just add a small signature at the end
            signature = f"GSA-{self.version}-L{clearance_level}".encode()
            return data + signature
            
        elif clearance_level <= 4:
            # Intermediate noise: insert noise at regular intervals
            noise_interval = max(64, len(data) // 8)
            result = bytearray()
            
            for i in range(0, len(data), noise_interval):
                # Add chunk of data
                chunk = data[i:min(i+noise_interval, len(data))]
                result.extend(chunk)
                
                # Add noise byte if not at the end
                if i + noise_interval < len(data):
                    # Deterministic but unpredictable noise based on position
                    noise_byte = (i ^ (i >> 8) ^ (i * 0x9E3779B9)) & 0xFF
                    result.append(noise_byte)
            
            return bytes(result)
            
        else:
            # Advanced noise: add variable-length noise blocks with patterns
            import hashlib
            import struct
            
            # Create a seed for noise generation
            seed = hashlib.sha256(data[:32]).digest()[:4]
            seed_value = int.from_bytes(seed, byteorder='little')
            
            import random
            noise_rng = random.Random(seed_value)
            
            # Insert noise blocks at semi-random positions
            result = bytearray(data)
            
            # Number of noise blocks to insert (proportional to data size)
            num_blocks = max(3, len(data) // 512)
            
            # Position markers for noise blocks (for removal)
            markers = []
            
            for i in range(num_blocks):
                # Position to insert noise (avoid existing markers)
                while True:
                    pos = noise_rng.randint(0, len(result) - 1)
                    # Check if position is far enough from existing markers
                    if all(abs(pos - m) > 16 for m in markers):
                        markers.append(pos)
                        break
                
                # Create a 8-16 byte noise block with a recognizable pattern
                noise_length = noise_rng.randint(8, 16)
                noise_block = bytearray(noise_length)
                
                # Fill with noise but include marker pattern
                for j in range(noise_length):
                    if j == 0:
                        noise_block[j] = 0xFE  # Start marker
                    elif j == noise_length - 1:
                        noise_block[j] = 0xEF  # End marker
                    else:
                        noise_block[j] = noise_rng.randint(0, 255)
                
                # Insert the noise block
                result[pos:pos] = noise_block
                
                # Update markers to account for inserted data
                for j in range(i+1, len(markers)):
                    if markers[j] >= pos:
                        markers[j] += noise_length
            
            return bytes(result)
    
    def _remove_noise_patterns(self, data: bytes, clearance_level: int) -> bytes:
        """Remove noise patterns added during encryption"""
        if clearance_level <= 2:
            # Basic noise: remove signature at the end
            signature = f"GSA-{self.version}-L{clearance_level}".encode()
            if data.endswith(signature):
                return data[:-len(signature)]
            return data
            
        elif clearance_level <= 4:
            # Intermediate noise: remove noise bytes at regular intervals
            noise_interval = max(64, len(data) // 8)
            
            # Calculate positions of noise bytes
            noise_positions = []
            pos = noise_interval
            while pos < len(data):
                noise_positions.append(pos)
                # Skip the original data chunk and the noise byte
                pos += noise_interval + 1
            
            # Remove noise bytes (starting from the end to preserve positions)
            result = bytearray(data)
            for pos in sorted(noise_positions, reverse=True):
                if pos < len(result):
                    del result[pos]
            
            return bytes(result)
            
        else:
            # Advanced noise: detect and remove noise blocks
            # Look for start marker (0xFE) and end marker (0xEF) pairs
            result = bytearray(data)
            
            # Find potential noise blocks
            i = 0
            while i < len(result) - 16:  # Maximum noise block size is 16
                if result[i] == 0xFE:
                    # Look for end marker within reasonable distance
                    for j in range(i+7, min(i+16, len(result))):  # Minimum 8, maximum 16
                        if result[j] == 0xEF:
                            # Found potential noise block
                            del result[i:j+1]
                            # Don't increment i, as we need to check the new byte at position i
                            break
                    else:
                        # No end marker found, move to next byte
                        i += 1
                else:
                    i += 1
            
            return bytes(result)
    
    def _normalize_length(self, data: bytes, clearance_level: int) -> bytes:
        """Normalize data length to standard blocks to prevent size-based identification"""
        # Different normalization strategies based on clearance level
        if clearance_level <= 3:
            # Basic padding to multiples of 16 bytes
            block_size = 16
            padding_needed = (block_size - len(data) % block_size) % block_size
            
            if padding_needed > 0:
                # Add PKCS#7 style padding
                padding = bytes([padding_needed] * padding_needed)
                return data + padding
            return data
            
        else:
            # Advanced normalization to fixed size bands
            # Identify the nearest size band and pad to it
            size_bands = [256, 512, 1024, 2048, 4096, 8192, 16384]
            
            target_size = None
            for band in size_bands:
                if len(data) <= band:
                    target_size = band
                    break
            
            if target_size is None:
                # If larger than the largest band, pad to the next multiple of largest band
                target_size = ((len(data) // size_bands[-1]) + 1) * size_bands[-1]
            
            padding_needed = target_size - len(data)
            
            # Use PKCS#7 style padding with a canary value to detect tampering
            padding = bytes([padding_needed & 0xFF] * padding_needed)
            
            return data + padding
    
    def _remove_normalization(self, data: bytes, clearance_level: int) -> bytes:
        """Remove length normalization padding"""
        if not data:
            return data
            
        # Check the last byte to determine padding length
        padding_size = data[-1]
        
        # Verify that padding is correct (all padding bytes should be the same)
        if padding_size > 0 and padding_size < 256:
            if all(b == padding_size for b in data[-padding_size:]):
                return data[:-padding_size]
        
        # If padding looks invalid, return the data unchanged
        return data


        